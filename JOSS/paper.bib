@article{Amari1993,
  title = {A universal theorem on learning curves},
  journal = {Neural Networks},
  volume = {6},
  number = {2},
  pages = {161-166},
  year = {1993},
  issn = {0893-6080},
  doi = {https://doi.org/10.1016/0893-6080(93)90013-M},
  url = {https://www.sciencedirect.com/science/article/pii/089360809390013M},
  author = {Shun-ichi Amari},
  keywords = {Learning curve, Generalization error, Entropic error, Information gain, Universal theorem},
  abstract = {A learning curve shows how fast a learning machine improves its behavior as the number of training examples increases. This paper proves a universal asymptotic behavior of learning curves for general noiseless dichotomy machines, or neural networks. It is proved that irrespective of the architecture of a machine, the average predictive entropy or the information gain 〈e∗(t)〉 converges to 0 as 〈e∗(t)〉 ∼ d/t as the number t of training exampies increases, where d is the number of modifiable parameters of a machine.}
}


@article{Amari1992,
  author = {Amari, Shun-ichi and Fujita, Naotake and Shinomoto, Shigeru},
  title = {Four Types of Learning Curves},
  journal = {Neural Computation},
  volume = {4},
  number = {4},
  pages = {605-618},
  year = {1992},
  month = {07},
  abstract = {If machines are learning to make decisions given a number of examples, the generalization error ε(t) is defined as the average probability that an incorrect decision is made for a new example by a machine when trained with t examples. The generalization error decreases as t increases, and the curve ε(t) is called a learning curve. The present paper uses the Bayesian approach to show that given the annealed approximation, learning curves can be classified into four asymptotic types. If the machine is deterministic with noiseless teacher signals, then (1) ε ∼ at-1 when the correct machine parameter is unique, and (2) ε ∼ at-2 when the set of the correct parameters has a finite measure. If the teacher signals are noisy, then (3) ε ∼ at-1/2 for a deterministic machine, and (4) ε ∼ c + at-1 for a stochastic machine.},
  issn = {0899-7667},
  doi = {10.1162/neco.1992.4.4.605},
  url = {https://doi.org/10.1162/neco.1992.4.4.605},
  eprint = {https://direct.mit.edu/neco/article-pdf/4/4/605/812352/neco.1992.4.4.605.pdf},
}


@Article{Haussler1996,
  author={Haussler, David
  and Kearns, Michael
  and Seung, H. Sebastian
  and Tishby, Naftali},
  title={Rigorous learning curve bounds from statistical mechanics},
  journal={Machine Learning},
  year={1996},
  month={Nov},
  day={01},
  volume={25},
  number={2},
  pages={195-236},
  abstract={In this paper we introduce and investigate a mathematically rigorous theory of learning curves that is based on ideas from statistical mechanics. The advantage of our theory over the well-established Vapnik-Chervonenkis theory is that our bounds can be considerably tighter in many cases, and are also more reflective of the true behavior of learning curves. This behavior can often exhibit dramatic properties such as phase transitions, as well as power law asymptotics not explained by the VC theory. The disadvantages of our theory are that its application requires knowledge of the input distribution, and it is limited so far to finite cardinality function classes.},
  issn={1573-0565},
  doi={10.1007/BF00114010},
  url={https://doi.org/10.1007/BF00114010}
}

@article{Hutter2021,
  author = {Marcus Hutter},
  title = {Learning Curve Theory},
  year = {2021},
  eprint = {arXiv:2102.04074},
  howpublished = {Latest 2021 version at http://www.hutter1.net/publ/scaling.pdf},
}

@article{schulz2022,
  title={Performance reserves in brain-imaging-based phenotype prediction},
  author={Schulz, Marc-Andre and Bzdok, Danilo and Haufe, Stefan and Haynes, John-Dylan and Ritter, Kerstin},
  journal={BioRxiv},
  pages={2022--02},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}