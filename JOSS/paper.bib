@article{Kamnitsas2017,
  title = {Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation},
  journal = {Medical Image Analysis},
  volume = {36},
  pages = {61-78},
  year = {2017},
  issn = {1361-8415},
  doi = {https://doi.org/10.1016/j.media.2016.10.004},
  url = {https://www.sciencedirect.com/science/article/pii/S1361841516301839},
  author = {Konstantinos Kamnitsas and Christian Ledig and Virginia F.J. Newcombe and Joanna P. Simpson and Andrew D. Kane and David K. Menon and Daniel Rueckert and Ben Glocker},
  keywords = {3D convolutional neural network, Fully connected CRF, Segmentation, Brain lesions, Deep learning},
  abstract = {We propose a dual pathway, 11-layers deep, three-dimensional Convolutional Neural Network for the challenging task of brain lesion segmentation. The devised architecture is the result of an in-depth analysis of the limitations of current networks proposed for similar applications. To overcome the computational burden of processing 3D medical scans, we have devised an efficient and effective dense training scheme which joins the processing of adjacent image patches into one pass through the network while automatically adapting to the inherent class imbalance present in the data. Further, we analyze the development of deeper, thus more discriminative 3D CNNs. In order to incorporate both local and larger contextual information, we employ a dual pathway architecture that processes the input images at multiple scales simultaneously. For post-processing of the network’s soft segmentation, we use a 3D fully connected Conditional Random Field which effectively removes false positives. Our pipeline is extensively evaluated on three challenging tasks of lesion segmentation in multi-channel MRI patient data with traumatic brain injuries, brain tumours, and ischemic stroke. We improve on the state-of-the-art for all three applications, with top ranking performance on the public benchmarks BRATS 2015 and ISLES 2015. Our method is computationally efficient, which allows its adoption in a variety of research and clinical settings. The source code of our implementation is made publicly available.}
}

@article{Plant2010,
  title={Automated detection of brain atrophy patterns based on MRI for the prediction of Alzheimer's disease},
  author={Claudia Plant and Stefan J. Teipel and Annahita Oswald and C. B{\"o}hm and Thomas Meindl and Janaina Mour{\~a}o Miranda and Arun L. W. Bokde and Harald Hampel and Michael Ewers},
  journal={Neuroimage},
  year={2010},
  volume={50},
  pages={162 - 174}
}

@Article{Woo2017,
  author={Woo, Choong-Wan
  and Chang, Luke J.
  and Lindquist, Martin A.
  and Wager, Tor D.},
  title={Building better biomarkers: brain models in translational neuroimaging},
  journal={Nature Neuroscience},
  year={2017},
  month={Mar},
  day={01},
  volume={20},
  number={3},
  pages={365-377},
  abstract={Neuroimaging and pattern recognition are being combined to develop brain models of clinical disorders. Such models yield biomarkers that can be shared and validated across populations, narrowing the gap between neuroscience and clinical applications. The authors summarize 475 translational modeling studies, highlighting challenges and ways to improve biomarker development.},
  issn={1546-1726},
  doi={10.1038/nn.4478},
  url={https://doi.org/10.1038/nn.4478}
}


@article{Amari1993,
  title = {A universal theorem on learning curves},
  journal = {Neural Networks},
  volume = {6},
  number = {2},
  pages = {161-166},
  year = {1993},
  issn = {0893-6080},
  doi = {https://doi.org/10.1016/0893-6080(93)90013-M},
  url = {https://www.sciencedirect.com/science/article/pii/089360809390013M},
  author = {Shun-ichi Amari},
  keywords = {Learning curve, Generalization error, Entropic error, Information gain, Universal theorem},
  abstract = {A learning curve shows how fast a learning machine improves its behavior as the number of training examples increases. This paper proves a universal asymptotic behavior of learning curves for general noiseless dichotomy machines, or neural networks. It is proved that irrespective of the architecture of a machine, the average predictive entropy or the information gain 〈e∗(t)〉 converges to 0 as 〈e∗(t)〉 ∼ d/t as the number t of training exampies increases, where d is the number of modifiable parameters of a machine.}
}


@article{Amari1992,
  author = {Amari, Shun-ichi and Fujita, Naotake and Shinomoto, Shigeru},
  title = {Four Types of Learning Curves},
  journal = {Neural Computation},
  volume = {4},
  number = {4},
  pages = {605-618},
  year = {1992},
  month = {07},
  abstract = {If machines are learning to make decisions given a number of examples, the generalization error ε(t) is defined as the average probability that an incorrect decision is made for a new example by a machine when trained with t examples. The generalization error decreases as t increases, and the curve ε(t) is called a learning curve. The present paper uses the Bayesian approach to show that given the annealed approximation, learning curves can be classified into four asymptotic types. If the machine is deterministic with noiseless teacher signals, then (1) ε ∼ at-1 when the correct machine parameter is unique, and (2) ε ∼ at-2 when the set of the correct parameters has a finite measure. If the teacher signals are noisy, then (3) ε ∼ at-1/2 for a deterministic machine, and (4) ε ∼ c + at-1 for a stochastic machine.},
  issn = {0899-7667},
  doi = {10.1162/neco.1992.4.4.605},
  url = {https://doi.org/10.1162/neco.1992.4.4.605},
  eprint = {https://direct.mit.edu/neco/article-pdf/4/4/605/812352/neco.1992.4.4.605.pdf},
}


@Article{Haussler1996,
  author={Haussler, David
  and Kearns, Michael
  and Seung, H. Sebastian
  and Tishby, Naftali},
  title={Rigorous learning curve bounds from statistical mechanics},
  journal={Machine Learning},
  year={1996},
  month={Nov},
  day={01},
  volume={25},
  number={2},
  pages={195-236},
  abstract={In this paper we introduce and investigate a mathematically rigorous theory of learning curves that is based on ideas from statistical mechanics. The advantage of our theory over the well-established Vapnik-Chervonenkis theory is that our bounds can be considerably tighter in many cases, and are also more reflective of the true behavior of learning curves. This behavior can often exhibit dramatic properties such as phase transitions, as well as power law asymptotics not explained by the VC theory. The disadvantages of our theory are that its application requires knowledge of the input distribution, and it is limited so far to finite cardinality function classes.},
  issn={1573-0565},
  doi={10.1007/BF00114010},
  url={https://doi.org/10.1007/BF00114010}
}

@article{Hutter2021,
  author = {Marcus Hutter},
  title = {Learning Curve Theory},
  year = {2021},
  eprint = {arXiv:2102.04074},
  howpublished = {Latest 2021 version at http://www.hutter1.net/publ/scaling.pdf},
}